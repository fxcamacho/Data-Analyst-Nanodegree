{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Wrangle OpenStreetMap \n",
    "\n",
    "## Author: Félix Carlos Camacho Criado\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#sum\">Summary</a></li>\n",
    "<li><a href=\"#map\">Map Area</a></li>\n",
    "<li><a href=\"#eda\">Audit phase</a></li>\n",
    "<li><a href=\"#dc\">Data Cleaning phase</a></li>\n",
    "<li><a href=\"#over\">Data Overview</a></li>\n",
    "<li><a href=\"#add\">Additional ideas about the dataset</a></li>\n",
    "<li><a href=\"#conc\">Conclusion</a></li>  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sum'></a>\n",
    "## Summary\n",
    "\n",
    ">In this project I will implement the techniques learnt in the Data Wrangling with SQL section. First, I will use the Overpass API to download a XML OSM dataset from https://www.openstreetmap.org. Then, I will proccess the dataset and audit it and remark the most noticeable findings and problems. After this, I will show some alternatives in order to\n",
    "fix these issues and update data in an appropiate way.\n",
    "\n",
    ">Once I have a clean dataset I will convert it from XML to CSV format, so, the cleaned .csv files can be import into a SQL database using the schema provided. After building my local database, there will be some time for data exploration and present additional ideas for improving the data quality of the dataset. In the last paragraph, you can find the conclusion that I draw after the project execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='map'></a>\n",
    "## Map Area\n",
    "\n",
    "### Location: Madrid, Metropolitan Area\n",
    "\n",
    ">I have choosen the metropolitan area of Madrid, because include info from my home and also from my workplace. This dataset not only include of Madrid city but info about all the cities which compose its metropolitan area as Getafe (the city where I live) so I am curious about which are the data included for this area and what can be improved.\n",
    "\n",
    ">Details about the location can be found in the following link: https://www.openstreetmap.org/relation/5326784\n",
    "\n",
    ">This file (OSM_XLM_Madrid_City_Map.osm) have of size of 867MB. Definetely it is not the best option for code testing, so I also use a sample of this place which I named sample_Madrid.osm of 3,2 MB and which is a more appropiate and agile option.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Audit Phase\n",
    "\n",
    "> During this phase, I will apply the audit techniques used in the case of study as this make me know about what kind of tags are in the dataset and which are the most common tags values. In a deeper audit of this dataset, I will remark the main issues identified and how they affect to the tags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will take a look to the unique elements that can be found in the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nd', 4896861),\n",
      " ('node', 3565274),\n",
      " ('tag', 2255104),\n",
      " ('way', 571743),\n",
      " ('member', 212575),\n",
      " ('relation', 13611),\n",
      " ('osm', 1),\n",
      " ('note', 1),\n",
      " ('meta', 1),\n",
      " ('bounds', 1)]\n"
     ]
    }
   ],
   "source": [
    "%run 1_eCounter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing the data and adding into the database, I will check \"k\" values and \"v\" value for each element and see if\n",
    "there are any potential problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the exercise provided in the case of study, I will check for certain patterns in the tags, so I have distribute\n",
    "all tags in four groups of regular expressions in order to know if the problematic characters are a significant number\n",
    ", which is not happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 1541385, 'lower_colon': 679739, 'other': 33977, 'problemchars': 3}\n"
     ]
    }
   ],
   "source": [
    "%run 2a_TagsK.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the \"v\" values for each tag, these are most common values for tags in Madrid. Due to there a lot of kind of tags I am showing only the ones repeated more than 50k times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('highway', 258195),\n",
      " ('building', 224060),\n",
      " ('name', 136735),\n",
      " ('addr:street', 90301),\n",
      " ('source', 89486),\n",
      " ('addr:housenumber', 88443),\n",
      " ('building:levels', 87952),\n",
      " ('addr:postcode', 71471),\n",
      " ('oneway', 58810),\n",
      " ('addr:city', 56975),\n",
      " ('natural', 53103),\n",
      " ('source:date', 52077),\n",
      " ('barrier', 51352)]\n"
     ]
    }
   ],
   "source": [
    "%run 2b_TagsV.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems encountered in the map\n",
    "I put the focus in the following key features: k = addr:street and k = addr:city. It seems that the users who contributed to include information of this area have used a different nomenclature for the kind of street and also for the names of cities and villages, therefore, I found several error that can be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auditing Street Names \n",
    "The main problems encountered for this key are:\n",
    "<ul>\n",
    "<li><b>LowerCase and Uppercase</b></li> E.g \"CARRETERA\" or \"carretera\" Instead of \"Carretera\".     \n",
    "<li><b>Abbreviations</b></li> E.g. \"CR\",\"Ctra\",\"CTRA.\" Instead of \"Carretera\".   \n",
    "<li><b>Misspelling</b></li> E.g. \"Pasage\" Instead of \"Pasaje\".   \n",
    "<li><b>Unwanted characters</b></li> E.g. \",\".   \n",
    "<li><b>Several ways to name the same concept </b></li> E.g. I found 11 strings for the Spanish word for \"Avenue\" (Avenida) such as \"A\",\"AV.\",\"AVDA.\",\"AVDA\",\"Avda.\",\"Avd.\" etc..\n",
    "</ul>\n",
    "\n",
    "I have identify the issues related with street names running the file \"3b_AuditStreetNames.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auditing City Names \n",
    "I found some similarities in the issues between this key (addr:city) and the previous one (addr:street) but I also found new problems to deal with.\n",
    "<ul>\n",
    "<li><b>LowerCase and Uppercase</b></li> E.g \"alcobendas\" Instead of \"Alcobendas\".\"MADRID\" Instead of \"Madrid\".  \n",
    "<li><b>Misspelling </b></li> E.g. \"Madrd\" Instead of \"Madrid\".\n",
    "<li><b>Unwanted characters</b></li> E.g. \",\".   \n",
    "<li><b>Uncomplete/Incorrect names for determined cities and villages </b></li> E.g. \"Rivas Vaciamadrid3\" Instead of \n",
    "\"Rivas-Vaciamadrid\"\n",
    "<li><b>Several way to name the same city.</b></li> E.g.\n",
    "\"Fuente el Saz de Jarama\",\"Fuente el Saz del Jarama\",\"FUENTE EL SAZ DE JARAMA\" Instead of  \"Fuente El Saz De Jarama\".    \n",
    "So, I tried to unified these names in one similar value.   \n",
    "\n",
    "</ul>\n",
    "\n",
    "I have identify the issues related with city names running the file \"3a_AudityCityNames.py\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dc'></a>\n",
    "## Data Cleaning phase\n",
    "\n",
    "> Once some issues have been identified, I cleaned data in order to correct these issues before convert and split the OSM file in CSV files. So I define a function for the update that will be called when the key was equal to 'addr:street' or 'addr:city'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    \n",
    "    unwanted = [',']  # List of unwanted characters \n",
    "    el = ''                  \n",
    " \n",
    "    #remove unwanted characters\n",
    "    for i in range(len(name)):\n",
    "        if name[i] not in unwanted:\n",
    "            el = el + name[i]\n",
    "\n",
    "    #Capitalize the first letter of each element and put to lower case the rest of letters\n",
    "    low_name = el.lower()\n",
    "    if ' ' in low_name:\n",
    "        el = ''\n",
    "        l = low_name.split(' ')\n",
    "        for i in l:\n",
    "            el = el + ' ' + i.capitalize()\n",
    "    else:\n",
    "        el = low_name.capitalize()\n",
    "\n",
    "    #Match with mapping dict and in case it found some know issue/value, it replaces it for the correct form.\n",
    "    k = mapping.keys()\n",
    "    key_list = list(k)\n",
    "    for abrev in key_list:\n",
    "        if abrev in el.split():\n",
    "            el = el.replace(abrev,mapping[abrev])\n",
    "\n",
    "    return el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The whole process can be found in the file 4_CleanUpdate.py. In addition, this process also launch the creation of five .CSV with info from nodes, ways and tags and their relations between them. Let's take a look in one .CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171946</td>\n",
       "      <td>crossing</td>\n",
       "      <td>traffic_signals</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171946</td>\n",
       "      <td>crossing_ref</td>\n",
       "      <td>zebra</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171946</td>\n",
       "      <td>highway</td>\n",
       "      <td>traffic_signals</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20952908</td>\n",
       "      <td>crossing</td>\n",
       "      <td>traffic_signals</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20952908</td>\n",
       "      <td>highway</td>\n",
       "      <td>crossing</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id           key            value     type\n",
       "0    171946      crossing  traffic_signals  regular\n",
       "1    171946  crossing_ref            zebra  regular\n",
       "2    171946       highway  traffic_signals  regular\n",
       "3  20952908      crossing  traffic_signals  regular\n",
       "4  20952908       highway         crossing  regular"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nodes_tags = pd.read_csv(\"nodes_tags.csv\") \n",
    "nodes_tags.head() #the header and values are which I expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now, .CSV files include cleaned data, next step is to create the db structure where the files will be imported. This step can be found in the file \"5_CreateDB.py\". Once the info is uploaded I will play some queries and try to obtain some basic statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='over'></a>\n",
    "## Data Overview\n",
    "\n",
    ">This section includes a information about the size of the .CSV files which has been uploaded to the file \"Madrid.db\". I will have a look into the file sizes, the number of nodes and ways, as well as, information about the users who contributed to fill the data about the metropolitan area of Madrid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ways_tags.csv        49.08 MB\n",
      "nodes_tags.csv       30.33 MB\n",
      "nodes.csv            299.47 MB\n",
      "Madrid.db            501.36 MB\n",
      "sample_Madrid.osm    3.24 MB\n",
      "OSM_XLM_Madrid_City_Map.osm 866.94 MB\n",
      "ways.csv             35.09 MB\n",
      "ways_nodes.csv       117.64 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "for root, dirs, files in os.walk(cwd+'/files_p2', topdown=False):\n",
    "    for name in files:\n",
    "        f = os.path.join(root, name)\n",
    "        print (name.ljust(20), round(os.path.getsize(f)/1000000, 2), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: [(3565273,)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "db = sqlite3.connect(\"Madrid.db\")\n",
    "c = db.cursor()\n",
    "\n",
    "# Fetch records from either Madrid.db\n",
    "QUERY = \"SELECT COUNT(*) FROM nodes;\"\n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"Number of nodes:\",rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ways: [(571741,)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"SELECT COUNT(*) FROM ways;\"\n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"Number of ways:\",rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: [(3534,)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"SELECT COUNT(DISTINCT(e.uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways)e\"\n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"Number of unique users:\",rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP 10 Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 Contributors: [('cirdancarpintero', 399112), ('carlosz22', 374452), ('rgbimport', 283903), ('robertogeb', 196791), ('rafaerti', 192873), ('gpesquero', 156083), ('mor', 131638), ('Iván_', 125767), ('sergionaranja', 96211), ('Cuenqui', 95691)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"SELECT e.user, COUNT(*) as num \\\n",
    "            FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "            GROUP BY e.user \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 10\"\n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"TOP 10 Contributors:\",rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users appearing only once (having 1 post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users with only one contribution [(984,)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"SELECT COUNT(*) \\\n",
    "           FROM (SELECT e.user, COUNT(*) as num \\\n",
    "           FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "           GROUP BY e.user \\\n",
    "           HAVING num=1)  u;\"\n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"Users with only one contribution\",rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which are the most common type of shops in this area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most typical kind of shop are : [('clothes', 1233), ('hairdresser', 1010), ('supermarket', 958), ('convenience', 856), ('bakery', 483), ('car_repair', 400), ('greengrocer', 383), ('shoes', 366), ('vacant', 306), ('optician', 283)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"SELECT value, COUNT(*) as num \\\n",
    "         FROM NODES_TAGS as nt \\\n",
    "         WHERE nt.key = 'shop'\\\n",
    "         GROUP BY nt.value \\\n",
    "         ORDER BY num DESC \\\n",
    "         LIMIT 10;\" \n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"Most typical kind of shop are :\",rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that clothing shops, hairdressers and supermarkets are the most typical shops that we can find in\n",
    "the metropolitan area of Madrid. Now we know that clothing stores are the kind of business more represented let see\n",
    "which are the brands behind these numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the clothing companies with more open stores in the Madrid metropolitan area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing brands with more presence in the dataset : [('Mango', 20), ('Zara', 17), ('H&M', 13), ('Sfera', 11), ('Springfield', 11), ('Calzedonia', 10), ('Cortefiel', 8), ('Humana', 8), ('Massimo Dutti', 8), ('Amichi', 7), ('C&A', 7), ('El Ganso', 7), ('Mulaya', 7), ('Oysho', 7), ('Primark', 7), ('Bershka', 6), ('Desigual', 6), ('Stradivarius', 6), ('Pilar Prieto', 5), ('Pull & Bear', 5)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"SELECT nt1.value, COUNT(nt1.id) as count \\\n",
    "         FROM nodes_tags nt1 \\\n",
    "         JOIN (SELECT id FROM nodes_tags WHERE value = 'clothes') nt2 \\\n",
    "         ON nt1.id = nt2.id \\\n",
    "         WHERE nt1.key = 'name' \\\n",
    "         GROUP BY nt1.value \\\n",
    "         ORDER BY count DESC \\\n",
    "         LIMIT 20;\"\n",
    "        \n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"Clothing brands with more presence in the dataset :\",rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can recognize a lot of very well know brands... for example the ones which belongs to Inditex Group (Zara, Massimo Duti, Bershka, Stradivarius, Oyso, Pull & Bear...) which is the biggest textile company in the world and also they are a Spanish company. In my opinion, it makes a lot of sense to see a lot of open stores of this group in its origin country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which are the TOP 5 Streets with more presence in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streets with more presence in the dataset : [(' Calle De Bravo Murillo', 346), (' Calle De Alcalá', 302), (' Calle De Fuencarral', 298), (' Calle De San Bernardo', 272), (' Calle Mayor', 243)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"SELECT value, COUNT(*) as num \\\n",
    "         FROM NODES_TAGS as nt \\\n",
    "         WHERE nt.key = 'street'\\\n",
    "         GROUP BY nt.value \\\n",
    "         ORDER BY num DESC \\\n",
    "         LIMIT 5;\" \n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"Streets with more presence in the dataset :\",rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result have sense, as \"Calle de Bravo Murillo\" and \"Calle de Alcalá\" are the two longest streets of Madrid, furthermore, \"Calle de Fuencarral\" is probably the busiest street of madrid and together with \"Calle de San Bernardo\" and \"Calle Mayor\" they belong to the city center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='add'></a>\n",
    "\n",
    "## Additional ideas about the dataset\n",
    "\n",
    ">As it was show in the basic stastics more than 3,5k users have added information to this area, in my opinion it is a considerable number of people and after some data exploration time I noticed that not all of them are using the same criteria when they include information.\n",
    "\n",
    ">We have some guidelines in the Wiki that should be followed by the users in order to have some consistency in the data but it seems that they are not followed by everyone.\n",
    "\n",
    ">One of the most shocking thing that I discovered is that the info from some keys are added in different language, in this dataset: Spanish and English. I can easily prove this having a look in the postal code key of ways_tags table. I downloaded the map of postal codes for Madrid region and I can ensure that all postal code in this area have a length of 5 characters and start with \"28XXX\". This can be checked in the following link: http://www.madrid.org/iestadis/fijas/estructu/general/territorio/descarga/cpos.pdf\n",
    "\n",
    ">So, I have added these conditions to the following sql query and I discovered that we have a significant number of keys with length = 5 and start with 28 whose key is \"codigo_postal\" which means \"postcode\" in Spanish.\n",
    "This query also allows me to indentify that besides to the language issue, they key \"postal_code\" is being used, so there is more than one problem to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kind of keys whose value have the following structure '28XXX' : [('postcode', 34471), ('codigo_postal', 911), ('postal_code', 119), ('ref', 58), ('housenumber', 3), ('city', 1), ('direction', 1), ('slope:direction', 1)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"SELECT key, COUNT(*) as num \\\n",
    "         FROM WAYS_TAGS as wt \\\n",
    "         WHERE (wt.value LIKE '%28%') AND (length(wt.value) = 5)\\\n",
    "         GROUP BY wt.key \\\n",
    "         ORDER BY num DESC;\"\n",
    "\n",
    "c.execute(QUERY)\n",
    "\n",
    "rows = c.fetchall()\n",
    "\n",
    "print(\"Kind of keys whose value have the following structure '28XXX' :\",rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">My proposal for avoid this situation and other related to this is select a reduced number of master users with more rights than the standard users for each area, so they will be responsible to do reviews about what other users have included. They will have the possibility to remove or accept what the standard users have propose to change. \n",
    "\n",
    ">The benefits for this proposal are:\n",
    "><ul>\n",
    "><li>Define a standarised way of working between the contributors for adding information to OpenStreetMap.\n",
    "><li>Data accuracy will improve as it is understood taht the master users will use the same criteria for completing the maps.\n",
    "><li>Improvements in keys categorization based in the experience of the master users which will be able to identify quicker the categorization issues of the elements.\n",
    "><li>Master users could easily identify the users more likely to include wrong information based in the number of contributions who they did to the map (if it is the first contribution, for sure, it should be checked) or based in the user previous contributions.\n",
    "\n",
    ">Nevertheless there some consideration to be taken into account in this solution. The master users should be trusted users because they will have the responsability to provide truthful information to the user which probably is one of the most important thing for OpenStreetMap. If the users cannot find reliable info, the project would have no value.\n",
    "\n",
    ">Another relevant topic to deal is the periodicity of the reviews, the master user role should demand a considerable activity of the master users in order to attend the modification requests from other standard users. If the request are put in a never ending queue or never be attended it will no see any improvement in the data quality for the maps. \n",
    "A potential solution to easily identify how valuable is a user it could be to introduce a user rating accesible to everyone in the same way than it is done in other platforms like Amazon or forums. I think this implementation would show information about the most valuable users who highly contribution to the development of the project.\n",
    "\n",
    ">In my opinion, these possible drawbacks would be resolved if these master rights were given to the TOP 10 contributors per area with a monthly basis review. This ranking could be elaborated measuring the number of contributions and also the rating provided by other users. The monthly basis review should be the lure for a actively contribution by the master users if they don't want to lose rights for modifying information over their favourites places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conc'></a>\n",
    "## Conclusion\n",
    "From my point of view, the current data from the dataset of the metropolitan city of Madrid, Spain shows valuable information that can be analyzed and used for making some general conclusions about what things we can find in this area. But if we want to show accurate information or present exact figures this is not the best dataset.\n",
    "\n",
    "I have found several issues in the data during the audit phase, specially related with the way about how the users include the information.I would say that the data quality of this file needs to be improved, validity and accuracy are not the best, probably this is because the high number of users which have contribute for this area (more than 3.5k) and considering that only around 28% (984 users) just contribute one time. Nevertheless, completeness of the file is quite good as we can find a lot of details about what can be found in the city.\n",
    "\n",
    "The good thing is that there is a large room of improvement and the number of users who contributed more than one time is high, so it is only a matter of time that this map present higher quality data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
